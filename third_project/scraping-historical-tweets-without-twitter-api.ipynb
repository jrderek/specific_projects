{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "resident-chemical",
   "metadata": {
    "papermill": {
     "duration": 0.021894,
     "end_time": "2021-04-15T20:36:13.806547",
     "exception": false,
     "start_time": "2021-04-15T20:36:13.784653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <center> Scraping historical tweets without a Twitter Developer Account\n",
    "\n",
    "![Image by Tumisu from Pixabay](https://mihaelagrigore.info/wp-content/uploads/2021/04/scrape-historical-tweets-and-other-social-networks.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-riding",
   "metadata": {
    "papermill": {
     "duration": 0.019831,
     "end_time": "2021-04-15T20:36:13.846405",
     "exception": false,
     "start_time": "2021-04-15T20:36:13.826574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The tool we will use:\n",
    "- snscrape\n",
    "\n",
    "What you need: \n",
    "- Python 3.8\n",
    "\n",
    "What you don't need:\n",
    "- a Twitter Developer Account\n",
    "\n",
    "\n",
    "For a research project related to public discourse about results on international large scale assessments I needed to scrape historical tweets, going back all the way to the begining of Twitter. This is how I discovered **snscrape**, a wonderful tool, easy to setup and use. \n",
    "\n",
    "I didn't find snscrape from the start, initially I was reading through the intricate details of Twitter Developer Account, application procedure, different levels of access, limits etc etc. But luckily a friend recommended snscrape and suddenly the task of collecting tweets became extremely easy.\n",
    "\n",
    "Snscrape is a popular tool with social scientists for Tweets collection, at least in 2021. Apparently, it bypasses several limitations of the Twitter API.  \n",
    "The prettiest thing is that you don't need Twitter developer account credentials (like you do with <a href='https://www.tweepy.org/'>Tweepy</a>, for example)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-sleeve",
   "metadata": {
    "papermill": {
     "duration": 0.021217,
     "end_time": "2021-04-15T20:36:13.888764",
     "exception": false,
     "start_time": "2021-04-15T20:36:13.867547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Table of contents\n",
    "\n",
    "\n",
    "1. [Installing snscrape](#1.-Installing-snscrape)\n",
    "2. [How to use snscrape](#2.-How-to-use-snscrape)\n",
    "3. [Calling snscrape CLI commands from Python Notebook](#3.-Calling-snscrape-CLI-commands-from-Python-Notebook)\n",
    "4. [Using snscrape Python wrapper](#4.-Using-snscrape-Python-wrapper)\n",
    "5. [Tweets meta-information gathered with snscrape](#5.-Tweets-meta-information-gathered-with-snscrape) \n",
    "6. [Dataset manipulation: JSON, CSV and Pandas DataFrame](#6.-Dataset-manipulation:-JSON,-CSV-and-Pandas-DataFrame)\n",
    "7. [Basic exploration of our collected dataset of tweets](#7.-Basic-exploration-of-our-collected-dataset-of-tweets)\n",
    "8. [Bonus: Publishing your Jupyter Notebook on Medium](#8.-Bonus:-Publishing-your-Jupyter-Notebook-on-Medium)\n",
    "9. [What next ? Sentiment analysis](#9.-What-next-?-Sentiment-analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-punch",
   "metadata": {
    "papermill": {
     "duration": 0.019919,
     "end_time": "2021-04-15T20:36:13.928744",
     "exception": false,
     "start_time": "2021-04-15T20:36:13.908825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We begin with some standard library imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "enclosed-grenada",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T20:36:13.974487Z",
     "iopub.status.busy": "2021-04-15T20:36:13.973457Z",
     "iopub.status.idle": "2021-04-15T20:36:13.978090Z",
     "shell.execute_reply": "2021-04-15T20:36:13.978633Z"
    },
    "papermill": {
     "duration": 0.028662,
     "end_time": "2021-04-15T20:36:13.978900",
     "exception": false,
     "start_time": "2021-04-15T20:36:13.950238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import uuid\n",
    "\n",
    "from IPython.display import display_javascript, display_html, display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, date, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-startup",
   "metadata": {
    "papermill": {
     "duration": 0.020757,
     "end_time": "2021-04-15T20:36:14.020124",
     "exception": false,
     "start_time": "2021-04-15T20:36:13.999367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Installing snscrape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-stadium",
   "metadata": {
    "papermill": {
     "duration": 0.021053,
     "end_time": "2021-04-15T20:36:14.063402",
     "exception": false,
     "start_time": "2021-04-15T20:36:14.042349",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Snscrape is available from its <a href='https://github.com/JustAnotherArchivist/snscrape'>official github project repository</a>.\n",
    "\n",
    "Snscrape has two versions:\n",
    "- released version, which you can install by running this line in a commant line terminal: **pip3 install snscrape** (for a Windows machine)\n",
    "- **development version**, which is said to have richer functionality, so this is the one I'll be using.   \n",
    "I will use the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-fitness",
   "metadata": {
    "papermill": {
     "duration": 0.019837,
     "end_time": "2021-04-15T20:36:14.103513",
     "exception": false,
     "start_time": "2021-04-15T20:36:14.083676",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "First, let's check the current Python version, as snscrape documentation mentions **it requires Python 3.8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "qualified-peace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T20:36:14.146935Z",
     "iopub.status.busy": "2021-04-15T20:36:14.146351Z",
     "iopub.status.idle": "2021-04-15T20:36:14.151087Z",
     "shell.execute_reply": "2021-04-15T20:36:14.151630Z"
    },
    "papermill": {
     "duration": 0.027976,
     "end_time": "2021-04-15T20:36:14.151796",
     "exception": false,
     "start_time": "2021-04-15T20:36:14.123820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.5\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-sweden",
   "metadata": {
    "papermill": {
     "duration": 0.020357,
     "end_time": "2021-04-15T20:36:14.193413",
     "exception": false,
     "start_time": "2021-04-15T20:36:14.173056",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If you don't see 3.8.x in your case, please upgrade your Python version before you continue this tutorial, otherwise you will **not be able to install snscrape**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-provision",
   "metadata": {
    "papermill": {
     "duration": 0.020484,
     "end_time": "2021-04-15T20:36:14.234583",
     "exception": false,
     "start_time": "2021-04-15T20:36:14.214099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Installing the development version of snscrape. \n",
    "This will not work when ran in Kaggle, because Kaggle only offers Python 3.7 for the time being"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "understood-station",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-04-15T20:36:14.293419Z",
     "iopub.status.busy": "2021-04-15T20:36:14.280492Z",
     "iopub.status.idle": "2021-04-15T20:36:24.044673Z",
     "shell.execute_reply": "2021-04-15T20:36:24.045087Z"
    },
    "papermill": {
     "duration": 9.790146,
     "end_time": "2021-04-15T20:36:24.045258",
     "exception": false,
     "start_time": "2021-04-15T20:36:14.255112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/JustAnotherArchivist/snscrape.git\n",
      "  Cloning https://github.com/JustAnotherArchivist/snscrape.git to /private/var/folders/v9/nv1t6g591m18kqp9zx75tqcc0000gn/T/pip-req-build-ucuevmlu\n",
      "  Running command git clone -q https://github.com/JustAnotherArchivist/snscrape.git /private/var/folders/v9/nv1t6g591m18kqp9zx75tqcc0000gn/T/pip-req-build-ucuevmlu\n",
      "Requirement already satisfied: requests[socks] in /opt/anaconda3/envs/notebook/lib/python3.9/site-packages (from snscrape==0.3.5.dev139+g35fb61a) (2.25.1)\n",
      "Collecting lxml\n",
      "  Downloading lxml-4.6.4-cp39-cp39-macosx_10_14_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 768 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.10.0-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 8.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.3.1-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/notebook/lib/python3.9/site-packages (from requests[socks]->snscrape==0.3.5.dev139+g35fb61a) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/envs/notebook/lib/python3.9/site-packages (from requests[socks]->snscrape==0.3.5.dev139+g35fb61a) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/notebook/lib/python3.9/site-packages (from requests[socks]->snscrape==0.3.5.dev139+g35fb61a) (2021.10.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/anaconda3/envs/notebook/lib/python3.9/site-packages (from requests[socks]->snscrape==0.3.5.dev139+g35fb61a) (4.0.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/anaconda3/envs/notebook/lib/python3.9/site-packages (from requests[socks]->snscrape==0.3.5.dev139+g35fb61a) (1.7.1)\n",
      "Building wheels for collected packages: snscrape\n",
      "  Building wheel for snscrape (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for snscrape: filename=snscrape-0.3.5.dev139+g35fb61a-py3-none-any.whl size=51842 sha256=8b948534dd4bd199cbcbff7d418ef5c12bf0d4d637b458ecbdf604d8ccab5416\n",
      "  Stored in directory: /private/var/folders/v9/nv1t6g591m18kqp9zx75tqcc0000gn/T/pip-ephem-wheel-cache-7dmv2inx/wheels/1a/ba/e2/39fa3a11802c4a622f2efc8be3f5ff854481051d0b4c95c1fd\n",
      "Successfully built snscrape\n",
      "Installing collected packages: soupsieve, lxml, beautifulsoup4, snscrape\n",
      "Successfully installed beautifulsoup4-4.10.0 lxml-4.6.4 snscrape-0.3.5.dev139+g35fb61a soupsieve-2.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/JustAnotherArchivist/snscrape.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "structured-eating",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-04-15T20:36:24.098320Z",
     "iopub.status.busy": "2021-04-15T20:36:24.097538Z",
     "iopub.status.idle": "2021-04-15T20:36:24.142682Z",
     "shell.execute_reply": "2021-04-15T20:36:24.142214Z"
    },
    "papermill": {
     "duration": 0.074091,
     "end_time": "2021-04-15T20:36:24.142883",
     "exception": false,
     "start_time": "2021-04-15T20:36:24.068792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painful-purse",
   "metadata": {
    "papermill": {
     "duration": 0.023295,
     "end_time": "2021-04-15T20:36:24.190543",
     "exception": false,
     "start_time": "2021-04-15T20:36:24.167248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. How to use snscrape\n",
    "\n",
    "- through its command line interface (CLI) in the command prompt terminal.\n",
    "- use Python to run the CLI commands from a Jupyter notebook, for example (if you don't want to use the terminal to run commands)\n",
    "- or use the official snscrape Python wrapper. The Python wrapper is not well documented, unfortunately.\n",
    "\n",
    "Parameters you can use:\n",
    "- --jsonl : get the data into jsonl format\n",
    "- --progress\n",
    "- --max-results : limit the number of tweets to collect\n",
    "- --with-entity : Include the entity (e.g. user, channel) as the first output item (default: False)\n",
    "- --since DATETIME : Only return results newer than DATETIME (default: None)\n",
    "- --progress : Report progress on stderr (default: False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "forbidden-ground",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-04-15T20:36:24.246421Z",
     "iopub.status.busy": "2021-04-15T20:36:24.245771Z",
     "iopub.status.idle": "2021-04-15T20:36:24.270813Z",
     "shell.execute_reply": "2021-04-15T20:36:24.270351Z"
    },
    "papermill": {
     "duration": 0.056726,
     "end_time": "2021-04-15T20:36:24.270969",
     "exception": false,
     "start_time": "2021-04-15T20:36:24.214243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: snscrape [-h] [--version] [-v] [--dump-locals] [--retry N] [-n N]\n",
      "                [-f FORMAT | --jsonl] [--with-entity] [--since DATETIME]\n",
      "                [--progress]\n",
      "                {telegram-channel,weibo-user,vkontakte-user,facebook-group,twitter-search,twitter-tweet,instagram-user,instagram-hashtag,instagram-location,reddit-user,reddit-subreddit,reddit-search,facebook-user,facebook-community,twitter-user,twitter-hashtag,twitter-list-posts,twitter-profile}\n",
      "                ...\n",
      "\n",
      "positional arguments:\n",
      "  {telegram-channel,weibo-user,vkontakte-user,facebook-group,twitter-search,twitter-tweet,instagram-user,instagram-hashtag,instagram-location,reddit-user,reddit-subreddit,reddit-search,facebook-user,facebook-community,twitter-user,twitter-hashtag,twitter-list-posts,twitter-profile}\n",
      "                        The scraper you want to use\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --version             show program's version number and exit\n",
      "  -v, --verbose, --verbosity\n",
      "                        Increase output verbosity (default: 0)\n",
      "  --dump-locals         Dump local variables on serious log messages (warnings\n",
      "                        or higher) (default: False)\n",
      "  --retry N, --retries N\n",
      "                        When the connection fails or the server returns an\n",
      "                        unexpected response, retry up to N times with an\n",
      "                        exponential backoff (default: 3)\n",
      "  -n N, --max-results N\n",
      "                        Only return the first N results (default: None)\n",
      "  -f FORMAT, --format FORMAT\n",
      "                        Output format (default: None)\n",
      "  --jsonl               Output JSONL (default: False)\n",
      "  --with-entity         Include the entity (e.g. user, channel) as the first\n",
      "                        output item (default: False)\n",
      "  --since DATETIME      Only return results newer than DATETIME (default:\n",
      "                        None)\n",
      "  --progress            Report progress on stderr (default: False)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Run the snscrape help to see what options / parameters we can use\n",
    "cmd = 'snscrape --help'\n",
    "\n",
    "#This is similar to running os.system(cmd), which would show the output of running the command in the Terminal\n",
    "#window from where I started my Jupyter Notebook (which is what I used to develop this code)\n",
    "#By using subprocees, I capture the commands's output into a variable, whose content I can then print here.\n",
    "output = subprocess.check_output(cmd, shell=True)\n",
    "                                 \n",
    "print(output.decode(\"utf-8\"))                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-tuition",
   "metadata": {
    "papermill": {
     "duration": 0.023868,
     "end_time": "2021-04-15T20:36:24.319234",
     "exception": false,
     "start_time": "2021-04-15T20:36:24.295366",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Calling snscrape CLI commands from Python Notebook\n",
    "\n",
    "Notice I make use of a few snscrape parameters:  \n",
    "- --max-results, to limit the search\n",
    "- --jsonl, to have my results saved directly into a json file\n",
    "- --since yyyy-mm-dd, so collect tweets starting with this date\n",
    "- twitter-search will tell snscrape what the actual text to search is.  \n",
    "    Notice I use the 'until:yyyy-mm-dd'. This is a workaround for the fact that sncrape does not have support for an --until DATETIME parameters.  \n",
    "    So I'm using Twitter's search <strong>until</strong> feature. That is, I am using a feature already built-in in Twitter search.  \n",
    "    For more <strong>search operators</strong> that you can use and pass on to snscrape as part of the text to search for, see the <a href='https://developer.twitter.com/en/docs/twitter-api/v1/rules-and-filtering/search-operators'>Twitter documentation on search operators</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "available-ecology",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-04-15T20:36:24.375634Z",
     "iopub.status.busy": "2021-04-15T20:36:24.375079Z",
     "iopub.status.idle": "2021-04-15T20:36:24.385522Z",
     "shell.execute_reply": "2021-04-15T20:36:24.384961Z"
    },
    "papermill": {
     "duration": 0.042259,
     "end_time": "2021-04-15T20:36:24.385660",
     "exception": false,
     "start_time": "2021-04-15T20:36:24.343401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#out_folder = '../working'\n",
    "#in_folder = '../input'\n",
    "\n",
    "#json_filename = out_folder + '/pisa2018-query-tweets.json'\n",
    "\n",
    "#Using the OS library to call CLI commands in Python\n",
    "#os.system(f'snscrape --max-results 5000 --jsonl --progress --since 2018-12-01 twitter-search \"#pisa2018 lang:fr until:2019-12-31\" > {json_filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-saver",
   "metadata": {
    "papermill": {
     "duration": 0.024112,
     "end_time": "2021-04-15T20:36:24.434259",
     "exception": false,
     "start_time": "2021-04-15T20:36:24.410147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Using snscrape Python wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dynamic-newcastle",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T20:36:24.488437Z",
     "iopub.status.busy": "2021-04-15T20:36:24.487864Z",
     "iopub.status.idle": "2021-04-15T20:36:24.490333Z",
     "shell.execute_reply": "2021-04-15T20:36:24.490735Z"
    },
    "papermill": {
     "duration": 0.032224,
     "end_time": "2021-04-15T20:36:24.490899",
     "exception": false,
     "start_time": "2021-04-15T20:36:24.458675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = date(2016, 12, 5)\n",
    "start = start.strftime('%Y-%m-%d')\n",
    "\n",
    "stop = date(2016, 12, 14)\n",
    "stop = stop.strftime('%Y-%m-%d')\n",
    "\n",
    "keyword = 'pisa2018'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "israeli-contents",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-04-15T20:36:24.556850Z",
     "iopub.status.busy": "2021-04-15T20:36:24.545810Z",
     "iopub.status.idle": "2021-04-15T20:36:24.563065Z",
     "shell.execute_reply": "2021-04-15T20:36:24.562480Z"
    },
    "papermill": {
     "duration": 0.047768,
     "end_time": "2021-04-15T20:36:24.563206",
     "exception": false,
     "start_time": "2021-04-15T20:36:24.515438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "maxTweets = 1000\n",
    "\n",
    "#We are going to write the data into a csv file\n",
    "#filename = out_folder + '/' + keyword + start + '-' + stop + '.csv'\n",
    "#csvFile = open(filename, 'a', newline='', encoding='utf8')\n",
    "\n",
    "#We write to the csv file by using csv writer\n",
    "#csvWriter = csv.writer(csvFile)\n",
    "#csvWriter.writerow(['id','date','tweet'])\n",
    "\n",
    "#I will use the following Twitter search operators:\n",
    "# since - start date for Tweets collection \n",
    "# stop  - stop date for Tweets collection\n",
    "# -filter:links - not very clear what this does, from Twitter search operators documentation: https://developer.twitter.com/en/docs/twitter-api/v1/rules-and-filtering/search-operators\n",
    "#                 but it looks like this will exclude tweets with links from the search results\n",
    "# -filter:replies - removes @reply tweets from search results\n",
    "#for i,tweet in enumerate(sntwitter.TwitterSearchScraper(keyword + 'since:' +  start + ' until:' + \\\n",
    "                                                        #stop + ' -filter:links -filter:replies').get_items()):\n",
    "    #if i > maxTweets :\n",
    "        #break\n",
    "    #csvWriter.writerow([tweet.id, tweet.date, tweet.content])\n",
    "\n",
    "#csvFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-liver",
   "metadata": {
    "papermill": {
     "duration": 0.024569,
     "end_time": "2021-04-15T20:36:24.612934",
     "exception": false,
     "start_time": "2021-04-15T20:36:24.588365",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Tweets meta-information gathered with snscrape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-demographic",
   "metadata": {
    "papermill": {
     "duration": 0.024366,
     "end_time": "2021-04-15T20:36:24.662390",
     "exception": false,
     "start_time": "2021-04-15T20:36:24.638024",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's have a look at all the information that is available for every single tweet scraped using snscrape.  \n",
    "\n",
    "For this code I am using one example file that I made precidely for this, which contains a single JSON object. If you want to use a JSON file created with the steps above, you need to make some changes before you can run json.loads on it, as explained in <a href='https://stackoverflow.com/questions/21058935/python-json-loads-shows-valueerror-extra-data'>this stackoverflow discussion</a>.\n",
    "\n",
    "The solution for pretty printing JSON data inside a Jupyter Notebook comes from <a href='https://gist.github.com/nerevar/a068ee373e22391ad3a1413b3e554fb5'>this github project</a>.\n",
    "\n",
    "Click on the + icons to expand the contents of that particular item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "charitable-senate",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T20:36:24.719433Z",
     "iopub.status.busy": "2021-04-15T20:36:24.718813Z",
     "iopub.status.idle": "2021-04-15T20:36:24.737582Z",
     "shell.execute_reply": "2021-04-15T20:36:24.736998Z"
    },
    "papermill": {
     "duration": 0.050474,
     "end_time": "2021-04-15T20:36:24.737718",
     "exception": false,
     "start_time": "2021-04-15T20:36:24.687244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/example/example.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ae2483b948d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../input/example/example.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/example/example.json'"
     ]
    }
   ],
   "source": [
    "filename = '../input/example/example.json'\n",
    "  \n",
    "with open(filename) as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "class RenderJSON(object):\n",
    "    def __init__(self, json_data):\n",
    "        if isinstance(json_data, dict) or isinstance(json_data, list):\n",
    "            self.json_str = json.dumps(json_data)\n",
    "        else:\n",
    "            self.json_str = json_data\n",
    "        self.uuid = str(uuid.uuid4())\n",
    "\n",
    "    def _ipython_display_(self):\n",
    "        display_html('<div id=\"{}\" style=\"color: #000000; background-color: #ffffff; height: 600px; width:100%;font: 12px/18px monospace !important;\"></div>'.format(self.uuid), raw=True)\n",
    "        display_javascript(\"\"\"\n",
    "        require([\"https://rawgit.com/caldwell/renderjson/master/renderjson.js\"], function() {\n",
    "            renderjson.set_show_to_level(2);\n",
    "            document.getElementById('%s').appendChild(renderjson(%s))\n",
    "        });\n",
    "      \"\"\" % (self.uuid, self.json_str), raw=True)\n",
    "\n",
    "RenderJSON([data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-congo",
   "metadata": {
    "papermill": {
     "duration": 0.025513,
     "end_time": "2021-04-15T20:36:24.789214",
     "exception": false,
     "start_time": "2021-04-15T20:36:24.763701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Dataset manipulation: JSON, CSV and Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-magnet",
   "metadata": {
    "papermill": {
     "duration": 0.025404,
     "end_time": "2021-04-15T20:36:24.840966",
     "exception": false,
     "start_time": "2021-04-15T20:36:24.815562",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Converting JSON to Pandas DataFrame\n",
    "\n",
    "Pandas DataFrame is **the** data structure of choice in Data Science, so we read the JSON file into a DataFrame.  \n",
    "\n",
    "Then we save it as CSV, since CSV is the most common file type for Data Science small projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "beginning-pocket",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T20:36:24.898537Z",
     "iopub.status.busy": "2021-04-15T20:36:24.897789Z",
     "iopub.status.idle": "2021-04-15T20:36:24.983314Z",
     "shell.execute_reply": "2021-04-15T20:36:24.982649Z"
    },
    "papermill": {
     "duration": 0.11607,
     "end_time": "2021-04-15T20:36:24.983475",
     "exception": false,
     "start_time": "2021-04-15T20:36:24.867405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-bb03ec8a69a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'pisa2018-query-tweets'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/pisa2018-keyword-in-tweeter-archive/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtweets_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/notebook/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/notebook/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/notebook/lib/python3.9/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/notebook/lib/python3.9/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    744\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0mdata_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/notebook/lib/python3.9/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/notebook/lib/python3.9/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/notebook/lib/python3.9/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m             )\n\u001b[1;32m   1142\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "filename = 'pisa2018-query-tweets'\n",
    "file = in_folder + '/pisa2018-keyword-in-tweeter-archive/' + filename\n",
    "tweets_df = pd.read_json(file +'.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-vector",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T20:36:25.040742Z",
     "iopub.status.busy": "2021-04-15T20:36:25.040179Z",
     "iopub.status.idle": "2021-04-15T20:36:25.044169Z",
     "shell.execute_reply": "2021-04-15T20:36:25.044601Z"
    },
    "papermill": {
     "duration": 0.034999,
     "end_time": "2021-04-15T20:36:25.044760",
     "exception": false,
     "start_time": "2021-04-15T20:36:25.009761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-deposit",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T20:36:25.100198Z",
     "iopub.status.busy": "2021-04-15T20:36:25.099631Z",
     "iopub.status.idle": "2021-04-15T20:36:25.136803Z",
     "shell.execute_reply": "2021-04-15T20:36:25.137271Z"
    },
    "papermill": {
     "duration": 0.066426,
     "end_time": "2021-04-15T20:36:25.137435",
     "exception": false,
     "start_time": "2021-04-15T20:36:25.071009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-beatles",
   "metadata": {
    "papermill": {
     "duration": 0.027621,
     "end_time": "2021-04-15T20:36:25.192258",
     "exception": false,
     "start_time": "2021-04-15T20:36:25.164637",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Saving DataFrame to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-jonathan",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T20:36:25.251466Z",
     "iopub.status.busy": "2021-04-15T20:36:25.250710Z",
     "iopub.status.idle": "2021-04-15T20:36:25.282977Z",
     "shell.execute_reply": "2021-04-15T20:36:25.282343Z"
    },
    "papermill": {
     "duration": 0.063773,
     "end_time": "2021-04-15T20:36:25.283132",
     "exception": false,
     "start_time": "2021-04-15T20:36:25.219359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets_df.to_csv(out_folder + '/' + filename +'.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-analysis",
   "metadata": {
    "papermill": {
     "duration": 0.026915,
     "end_time": "2021-04-15T20:36:25.337161",
     "exception": false,
     "start_time": "2021-04-15T20:36:25.310246",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Basic exploration of our collected dataset of tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-invitation",
   "metadata": {
    "papermill": {
     "duration": 0.026732,
     "end_time": "2021-04-15T20:36:25.391036",
     "exception": false,
     "start_time": "2021-04-15T20:36:25.364304",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Basic introduction to tweets\n",
    "\n",
    "Tweets are 280 character messages (hence the name 'microblogging'). Just like on other social media platforms, you need to create an account and then you can start participating to the tweetverse.  \n",
    "\n",
    "Tweets act as short status updates. Tweets appear on timelines. Timelines are collections of tweets sorted in a chronological order. On your account's home page, you're shown a timeline where tweets from people you follow will be displayed. \n",
    "\n",
    "You can post your own brand new tweet, retweet an already existing tweet (which means ou just share the exact same tweet) or quote an existing tweet (similar to retweeting, but you can add your own comment to it). \n",
    "\n",
    "You can also reply to someone else's tweets or 'like' them.  \n",
    "\n",
    "Tweets often contain **entities**, which are mentions of:\n",
    "- other users, which appear in the form of @other_user\n",
    "- places\n",
    "- urls\n",
    "- media that was attached to the tweet\n",
    "- hashtags, that look like #example_hashtag. Hashtags are just a way to apply a label on a tweet. If I'm tweeting something about results of PISA, the Programme for International Student Assessment, I will likely use #oecdpisa in my tweet, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-adapter",
   "metadata": {
    "papermill": {
     "duration": 0.026781,
     "end_time": "2021-04-15T20:36:25.444769",
     "exception": false,
     "start_time": "2021-04-15T20:36:25.417988",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Counting the number of Tweets we scraped  \n",
    "\n",
    "The following cell is overkill in this particular scenario, but imagine you just scraped 1 million tweets and you want to know how many you got. The cell below is a very efficient way to count in that case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-messenger",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T20:36:25.504043Z",
     "iopub.status.busy": "2021-04-15T20:36:25.503413Z",
     "iopub.status.idle": "2021-04-15T20:36:25.509849Z",
     "shell.execute_reply": "2021-04-15T20:36:25.509363Z"
    },
    "papermill": {
     "duration": 0.037837,
     "end_time": "2021-04-15T20:36:25.510026",
     "exception": false,
     "start_time": "2021-04-15T20:36:25.472189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num = sum(1 for line in open(file +'.json'))\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-finland",
   "metadata": {
    "papermill": {
     "duration": 0.027293,
     "end_time": "2021-04-15T20:36:25.565073",
     "exception": false,
     "start_time": "2021-04-15T20:36:25.537780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Check tweets for a particular text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-currency",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T20:36:25.623371Z",
     "iopub.status.busy": "2021-04-15T20:36:25.622799Z",
     "iopub.status.idle": "2021-04-15T20:36:25.633469Z",
     "shell.execute_reply": "2021-04-15T20:36:25.632672Z"
    },
    "papermill": {
     "duration": 0.040892,
     "end_time": "2021-04-15T20:36:25.633669",
     "exception": false,
     "start_time": "2021-04-15T20:36:25.592777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "substring = 'justesse'\n",
    "\n",
    "count = 0\n",
    "f = open(file + '.json', 'r')\n",
    "for i, line in enumerate(f):\n",
    "    if substring in line:\n",
    "        count = count + 1\n",
    "        obj = json.loads(line)\n",
    "        print(f'Tweet number {count}: {obj[\"content\"]}')\n",
    "print(count)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-apartment",
   "metadata": {
    "papermill": {
     "duration": 0.028312,
     "end_time": "2021-04-15T20:36:25.691572",
     "exception": false,
     "start_time": "2021-04-15T20:36:25.663260",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The actual content of the tweet is available through test_df['content'] or test_df.content  \n",
    "\n",
    "renderedContent seems to contain the same information as content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-steps",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T20:36:25.755110Z",
     "iopub.status.busy": "2021-04-15T20:36:25.754366Z",
     "iopub.status.idle": "2021-04-15T20:36:25.758398Z",
     "shell.execute_reply": "2021-04-15T20:36:25.757933Z"
    },
    "papermill": {
     "duration": 0.03796,
     "end_time": "2021-04-15T20:36:25.758530",
     "exception": false,
     "start_time": "2021-04-15T20:36:25.720570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets_df.iloc[0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-practice",
   "metadata": {
    "papermill": {
     "duration": 0.028131,
     "end_time": "2021-04-15T20:36:25.814664",
     "exception": false,
     "start_time": "2021-04-15T20:36:25.786533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Links mentioned in the tweet are also listed separately in the outlinks column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-shore",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T20:36:25.877303Z",
     "iopub.status.busy": "2021-04-15T20:36:25.876483Z",
     "iopub.status.idle": "2021-04-15T20:36:25.880818Z",
     "shell.execute_reply": "2021-04-15T20:36:25.880283Z"
    },
    "papermill": {
     "duration": 0.037951,
     "end_time": "2021-04-15T20:36:25.880984",
     "exception": false,
     "start_time": "2021-04-15T20:36:25.843033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets_df.iloc[0].outlinks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-immunology",
   "metadata": {
    "papermill": {
     "duration": 0.029288,
     "end_time": "2021-04-15T20:36:25.940049",
     "exception": false,
     "start_time": "2021-04-15T20:36:25.910761",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can gauge the popularity of a tweet through these features:\n",
    "- replyCount\n",
    "- retweetCount\n",
    "- likeCount\n",
    "- quoteCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-appraisal",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T20:36:26.000316Z",
     "iopub.status.busy": "2021-04-15T20:36:25.999613Z",
     "iopub.status.idle": "2021-04-15T20:36:26.007643Z",
     "shell.execute_reply": "2021-04-15T20:36:26.007190Z"
    },
    "papermill": {
     "duration": 0.039321,
     "end_time": "2021-04-15T20:36:26.007776",
     "exception": false,
     "start_time": "2021-04-15T20:36:25.968455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "popularity_columns = ['replyCount', 'retweetCount', 'likeCount', 'quoteCount']\n",
    "tweets_df.iloc[0][popularity_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-celtic",
   "metadata": {
    "papermill": {
     "duration": 0.029852,
     "end_time": "2021-04-15T20:36:26.066356",
     "exception": false,
     "start_time": "2021-04-15T20:36:26.036504",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Find the most retweeted tweet in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-harris",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-15T20:36:26.131518Z",
     "iopub.status.busy": "2021-04-15T20:36:26.130946Z",
     "iopub.status.idle": "2021-04-15T20:36:26.134794Z",
     "shell.execute_reply": "2021-04-15T20:36:26.134192Z"
    },
    "papermill": {
     "duration": 0.039931,
     "end_time": "2021-04-15T20:36:26.134971",
     "exception": false,
     "start_time": "2021-04-15T20:36:26.095040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets_df.iloc[tweets_df.retweetCount.idxmax()][['content','retweetCount']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-copying",
   "metadata": {
    "papermill": {
     "duration": 0.029013,
     "end_time": "2021-04-15T20:36:26.193082",
     "exception": false,
     "start_time": "2021-04-15T20:36:26.164069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Bonus: Publishing your Jupyter Notebook on Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-publicity",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-04-15T20:36:26.258428Z",
     "iopub.status.busy": "2021-04-15T20:36:26.255226Z",
     "iopub.status.idle": "2021-04-15T20:36:34.744926Z",
     "shell.execute_reply": "2021-04-15T20:36:34.745351Z"
    },
    "papermill": {
     "duration": 8.523554,
     "end_time": "2021-04-15T20:36:34.745521",
     "exception": false,
     "start_time": "2021-04-15T20:36:26.221967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install jupyter_to_medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-university",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-04-15T20:36:34.819041Z",
     "iopub.status.busy": "2021-04-15T20:36:34.818249Z",
     "iopub.status.idle": "2021-04-15T20:36:34.821998Z",
     "shell.execute_reply": "2021-04-15T20:36:34.822524Z"
    },
    "papermill": {
     "duration": 0.043699,
     "end_time": "2021-04-15T20:36:34.822681",
     "exception": false,
     "start_time": "2021-04-15T20:36:34.778982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#uncomment and customize the code below in order to publish your notebook on your Medium account\n",
    "'''\n",
    "import jupyter_to_medium as jtm\n",
    "jtm.publish('Scraping historical tweets without a Twitter Developer Account.ipynb',\n",
    "            integration_token='paste_your_own_token',\n",
    "            pub_name=None,\n",
    "            title='Scraping historical tweets without a Twitter Developer Account',\n",
    "            tags=['scraping with Python', 'Twitter archive'],\n",
    "            publish_status='draft',\n",
    "            notify_followers=False,\n",
    "            license='all-rights-reserved',\n",
    "            canonical_url=None,\n",
    "            chrome_path=None,\n",
    "            save_markdown=False,\n",
    "            table_conversion='chrome'\n",
    "            )\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-vatican",
   "metadata": {
    "papermill": {
     "duration": 0.033398,
     "end_time": "2021-04-15T20:36:34.889491",
     "exception": false,
     "start_time": "2021-04-15T20:36:34.856093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "And that's about it for a quick intro to scraping tweets without the need to apply for a Twitter Developer Account and with no limitations for the maximum number of tweets we can get or for how far back in time we can go.\n",
    "\n",
    "## 9. What next ? Sentiment analysis\n",
    "\n",
    "What to do next with the tweets you just scraped ? In my case, I was very interested in <a href='https://www.kaggle.com/mishki/twitter-sentiment-analysis-using-nlp-techniques'>NLP for sentiment analysis of tweets</a>, or you may try topic modelling using Latent Dirichlet Allocation (LDA) or build a network graph from this data and use network analysis methods on it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27.340368,
   "end_time": "2021-04-15T20:36:35.532664",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-15T20:36:08.192296",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
